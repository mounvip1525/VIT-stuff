import pandas as pd
import re
import torch
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from transformers import DistilBertTokenizerFast, SentenceTransformer, util

# Load the dataset from Excel file (replace with your file path)
df = pd.read_excel("your_dataset.xlsx")

# Define relevant columns for preprocessing
columns_to_preprocess = ["vendor name", "product name", "category", "sub category", "product description", "BPPA Domain", "BPPA Sub domain", "BPPA Sub Sub domain", "Strategic outlook"]

# Combine information from selected columns into a single string
df["combined_info"] = df[columns_to_preprocess].apply(lambda x: ' '.join(x.astype(str)), axis=1)

# Define weights for each column
weight_dict = {
    "vendor name": 0.1,
    "product name": 0.1,
    "category": 0.2,
    "sub category": 0.1,
    "product description": 0.3,
    "BPPA Domain": 0.1,
    "BPPA Sub domain": 0.05,
    "BPPA Sub Sub domain": 0.05,
    "Strategic outlook": 0.05
}

# Define custom stopwords list from NLTK
custom_stopwords = set(stopwords.words('english'))

def preprocess_text(text, weights):
    """
    Preprocesses text for search, handling potential sentences within descriptions.

    Args:
        text: A string containing text to be preprocessed.
        weights: A dictionary where keys are column names and values are weights.

    Returns:
        A string (or list of strings) containing the preprocessed text.
    """
    text = text.lower()  # Convert text to lowercase

    # Remove non-alphanumeric characters and punctuation
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    # Tokenize text using NLTK
    tokens = word_tokenize(text)

    # Remove stopwords and perform stemming
    ps = PorterStemmer()
    weighted_words = [ps.stem(token) * weights.get('word', 1) for token in tokens if token.lower() not in custom_stopwords]

    # Join weighted words into text
    weighted_text = ' '.join(weighted_words)

    return weighted_text

# Apply preprocessing
df["processed_info"] = df["combined_info"].apply(lambda x: preprocess_text(x, weight_dict))

# Initialize DistilBERT tokenizer and model
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
model = SentenceTransformer("distilbert-base-uncased")

def encode_text(text, tokenizer, model):
    """
    Encodes text using the DistilBERT model.

    Args:
        text: A string or list of strings containing text to be encoded.
        tokenizer: A tokenizer object for tokenizing the text.
        model: A SentenceTransformer model for encoding the text.

    Returns:
        A tensor containing the encoded representations of the input text.
    """
    # Tokenize text
    tokenized_text = tokenizer(text, padding=True, truncation=True, return_tensors="pt")

    # Encode text
    with torch.no_grad():
        model_output = model(**tokenized_text)

    # Extract embeddings
    embeddings = model_output["sentence_embedding"]

    return embeddings

# Apply encoding to preprocessed text
df["encoded_info"] = df["processed_info"].apply(lambda x: encode_text(x, tokenizer, model))

def search(query, encoded_data, top_k=5):
    """
    Searches for similar items based on a query.

    Args:
        query (str): The user's search query.
        encoded_data (torch.Tensor): Encoded representations of the dataset items.
        top_k (int): Number of top results to return.

    Returns:
        list: A list of tuples containing (index, score) pairs of the most similar items.
    """
    # Encode the query
    query_embedding = model.encode(query, convert_to_tensor=True)

    # Calculate cosine similarity between the query and dataset items
    cosine_scores = util.pytorch_cos_sim(query_embedding, encoded_data)[0]

    # Get indices and scores of top k most similar items
    top_results = torch.topk(cosine_scores, k=top_k)

    # Convert indices and scores to a list of tuples
    results = [(idx.item(), score.item()) for idx, score in zip(top_results.indices, top_results.values)]
    
    return results

# Example usage:
# Assuming you have encoded_info as encoded representations of your dataset items
encoded_data = df["encoded_info"].tolist()  # Convert DataFrame column to list of tensors
query = "customer campaign management and compliance"
results = search(query, encoded_data)
print(results)
